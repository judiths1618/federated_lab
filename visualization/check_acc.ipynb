{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a89cae0c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n\n",
        "PROJECT_ROOT = Path.cwd().resolve().parent\n",
        "RUNS_ROOT = PROJECT_ROOT / 'runs'\n",
        "DATA_ROOT = PROJECT_ROOT / 'data' / 'MNIST'\n",
        "print('Looking for runs in:', RUNS_ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "444ef340",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_all_runs(runs_root: Path) -> pd.DataFrame:\n",
        "    runs_root = runs_root if runs_root.name == \"runs\" else (runs_root / \"runs\")\n",
        "    rows = []\n",
        "    if not runs_root.exists():\n",
        "        print(\"No runs folder found:\", runs_root)\n",
        "        return pd.DataFrame()\n",
        "    for exp_dir in sorted(runs_root.glob(\"*\")):\n",
        "        if not exp_dir.is_dir():\n",
        "            continue\n",
        "        for ts_dir in sorted(exp_dir.glob(\"*\")):\n",
        "            csv_path = ts_dir / \"fl_log.csv\"\n",
        "            if csv_path.exists():\n",
        "                try:\n",
        "                    df = pd.read_csv(csv_path)\n",
        "                    df[\"exp\"] = exp_dir.name\n",
        "                    df[\"ts\"] = ts_dir.name\n",
        "                    rows.append(df)\n",
        "                except Exception as e:\n",
        "                    print(\"Failed to read\", csv_path, e)\n",
        "    if not rows:\n",
        "        return pd.DataFrame()\n",
        "    return pd.concat(rows, ignore_index=True)\n",
        "\n",
        "df = load_all_runs(RUNS_ROOT)\n",
        "print(\"Loaded rows:\", len(df))\n",
        "df.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dcd1f07",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Federated evaluation across rounds & nodes using base + delta reconstruction.\n",
        "# - Auto-discovers files like:\n",
        "#     models/global_round_{r}_base.pt (or fallback: global_round_{r}.pt)\n",
        "#     updates/round_{r}_node_{i}_delta.pt\n",
        "# - Reconstructs each node's local model: base + delta\n",
        "# - Evaluates accuracy on a test loader (replace with your real test loader)\n",
        "# - Summarizes results in a DataFrame and saves CSV\n",
        "#\n",
        "# If no artifacts are found at the default path, this will run a synthetic demo\n",
        "# so you can see the full pipeline and results format.\n",
        "#\n",
        "# === HOW TO ADAPT ===\n",
        "# 1) Set RUN_DIR to your run folder containing \"models\" and \"updates\".\n",
        "# 2) Replace MODEL_FACTORY() with your actual model constructor.\n",
        "# 3) Replace build_test_loader() with your real test DataLoader.\n",
        "# 4) If your .pt files are full models (not state_dict), adjust load logic.\n",
        "#\n",
        "# Files will be saved to /mnt/data, including results CSV.\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "from typing import Dict, Any\n",
        "# -----------------------------\n",
        "# Config: paths\n",
        "# -----------------------------\n",
        "# Try to auto-detect a likely run dir under /mnt/data/runs/benign/*\n",
        "CANDIDATE = None\n",
        "runs_root = RUNS_ROOT / \"benign\"\n",
        "if runs_root.exists():\n",
        "    # pick the most recently modified child that has both models/ and updates/\n",
        "    candidates = []\n",
        "    for p in runs_root.glob(\"*\"):\n",
        "        if p.is_dir() and (p / \"models\").exists() and (p / \"updates\").exists():\n",
        "            candidates.append((p.stat().st_mtime, p))\n",
        "    if candidates:\n",
        "        CANDIDATE = sorted(candidates, key=lambda x: x[0], reverse=True)[0][1]\n",
        "# Allow the user to change this path if needed:\n",
        "RUN_DIR = CANDIDATE if CANDIDATE else (runs_root / \"20250813-224933\")\n",
        "MODELS_DIR = RUN_DIR / \"models\"\n",
        "UPDATES_DIR = RUN_DIR / \"updates\"\n",
        "# -----------------------------\n",
        "# Config: model + test loader\n",
        "# -----------------------------\n",
        "NUM_CLASSES = 10\n",
        "IMAGE_SHAPE = (1, 28, 28)\n",
        "from flsim.models import LinearMNIST\n",
        "from torchvision import datasets, transforms\n",
        "def MODEL_FACTORY():\n",
        "    return LinearMNIST()\n",
        "def build_test_loader():\n",
        "    transform = transforms.ToTensor()\n",
        "    dataset = datasets.MNIST(root=DATA_ROOT, train=False, download=False, transform=transform)\n",
        "    subset = torch.utils.data.Subset(dataset, range(1000))\n",
        "    return DataLoader(subset, batch_size=64)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "def reconstruct_full_state(base_state: Dict[str, torch.Tensor],\n",
        "                           delta_state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
        "    full = {}\n",
        "    for k, base_tensor in base_state.items():\n",
        "        if k in delta_state:\n",
        "            full[k] = base_tensor + delta_state[k]\n",
        "        else:\n",
        "            full[k] = base_tensor\n",
        "    for k, v in delta_state.items():\n",
        "        if k not in full:\n",
        "            full[k] = v\n",
        "    return full\n",
        "@torch.no_grad()\n",
        "def evaluate(model: nn.Module, loader: DataLoader) -> Dict[str, Any]:\n",
        "    model.eval()\n",
        "    total, correct, total_loss = 0, 0, 0.0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == yb).sum().item()\n",
        "        total += yb.numel()\n",
        "        total_loss += loss.item() * yb.size(0)\n",
        "    acc = correct / total if total > 0 else 0.0\n",
        "    avg_loss = total_loss / total if total > 0 else 0.0\n",
        "    return {\"acc\": acc, \"loss\": avg_loss, \"n\": total}\n",
        "def find_rounds_nodes(updates_dir: Path):\n",
        "    # pattern: round_{r}_node_{i}_delta.pt\n",
        "    pat = re.compile(r\"round_(\\d+)_node_(\\d+)_delta\\.pt$\")\n",
        "    mapping = {}\n",
        "    if not updates_dir.exists():\n",
        "        return mapping\n",
        "    for f in updates_dir.glob(\"*.pt\"):\n",
        "        m = pat.search(f.name)\n",
        "        if m:\n",
        "            r = int(m.group(1))\n",
        "            i = int(m.group(2))\n",
        "            mapping.setdefault(r, []).append((i, f))\n",
        "    # sort nodes by id per round\n",
        "    for r in mapping:\n",
        "        mapping[r] = sorted(mapping[r], key=lambda x: x[0])\n",
        "    return dict(sorted(mapping.items(), key=lambda x: x[0]))\n",
        "def load_base_for_round(models_dir: Path, r: int):\n",
        "    # prefer global_round_{r}_base.pt else fallback to global_round_{r}.pt\n",
        "    base1 = models_dir / f\"global_round_{r-1}_base.pt\"\n",
        "    base2 = models_dir / f\"global_round_{r-1}.pt\"\n",
        "    if base1.exists():\n",
        "        return torch.load(base1, map_location=\"cpu\")\n",
        "    if base2.exists():\n",
        "        return torch.load(base2, map_location=\"cpu\")\n",
        "    return None\n",
        "# -----------------------------\n",
        "# Main evaluation\n",
        "# -----------------------------\n",
        "test_loader = build_test_loader()\n",
        "results = []\n",
        "artifact_found = MODELS_DIR.exists() and UPDATES_DIR.exists()\n",
        "if artifact_found:\n",
        "    rounds = find_rounds_nodes(UPDATES_DIR)\n",
        "    if not rounds:\n",
        "        print(f\"[Info] No update deltas found under {UPDATES_DIR}. Running synthetic demo instead.\")\n",
        "        artifact_found = False\n",
        "if artifact_found:\n",
        "    print(f\"[Using artifacts from] {RUN_DIR}\")\n",
        "    for r, node_list in rounds.items():\n",
        "        base_state = load_base_for_round(MODELS_DIR, r)\n",
        "        if base_state is None:\n",
        "            print(f\"[Warn] No base model found for round {r} in {MODELS_DIR}, skipping this round.\")\n",
        "            continue\n",
        "        for node_id, delta_path in node_list:\n",
        "            try:\n",
        "                delta_state = torch.load(delta_path, map_location=\"cpu\")\n",
        "            except Exception as e:\n",
        "                print(f\"[Error] Could not load delta {delta_path}: {e}\")\n",
        "                continue\n",
        "            # reconstruct\n",
        "            reconstructed_state = reconstruct_full_state(base_state, delta_state)\n",
        "            model = MODEL_FACTORY().to(device)\n",
        "            try:\n",
        "                missing, unexpected = model.load_state_dict(reconstructed_state, strict=False)\n",
        "            except Exception as e:\n",
        "                print(f\"[Error] load_state_dict failed for round {r}, node {node_id}: {e}\")\n",
        "                continue\n",
        "            # evaluate\n",
        "            metrics = evaluate(model, test_loader)\n",
        "            results.append({\n",
        "                \"round\": r,\n",
        "                \"node\": node_id,\n",
        "                \"acc\": metrics[\"acc\"],\n",
        "                \"loss\": metrics[\"loss\"],\n",
        "                \"n\": metrics[\"n\"],\n",
        "                \"missing_keys\": len(missing),\n",
        "                \"unexpected_keys\": len(unexpected),\n",
        "                \"delta_file\": str(delta_path.name)\n",
        "            })\n",
        "else:\n",
        "    # Synthetic demo so the pipeline is visible and results are produced\n",
        "    print(\"[Demo] No real artifacts detected. Running a synthetic demonstration.\")\n",
        "    # Create a base model\n",
        "    base_model = MODEL_FACTORY().to(device)\n",
        "    base_state = base_model.state_dict()\n",
        "    # Simulate 2 rounds \u00d7 3 nodes with small random deltas\n",
        "    for r in range(2):\n",
        "        for node_id in range(3):\n",
        "            delta_state = {k: torch.randn_like(v) * 0.01 for k, v in base_state.items()}\n",
        "            reconstructed_state = reconstruct_full_state(base_state, delta_state)\n",
        "            model = MODEL_FACTORY().to(device)\n",
        "            model.load_state_dict(reconstructed_state, strict=False)\n",
        "            metrics = evaluate(model, test_loader)\n",
        "            results.append({\n",
        "                \"round\": r,\n",
        "                \"node\": node_id,\n",
        "                \"acc\": metrics[\"acc\"],\n",
        "                \"loss\": metrics[\"loss\"],\n",
        "                \"n\": metrics[\"n\"],\n",
        "                \"missing_keys\": 0,\n",
        "                \"unexpected_keys\": 0,\n",
        "                \"delta_file\": f\"synthetic_round_{r}_node_{node_id}_delta\"\n",
        "            })\n",
        "# -----------------------------\n",
        "# Save & show results\n",
        "# -----------------------------\n",
        "df = pd.DataFrame(results).sort_values([\"round\", \"node\"]).reset_index(drop=True)\n",
        "print(df)\n",
        "# out_csv = \"/mnt/data/per_node_accuracy_by_round.csv\"\n",
        "# df.to_csv(out_csv, index=False)\n",
        "# Display table to the user in the UI\n",
        "# %pip install caas_jupyter_tools\n",
        "# from caas_jupyter_tools import display_dataframe_to_user\n",
        "# display_dataframe_to_user(\"Per-node accuracy by round\", df)\n",
        "# print(f\"\\nSaved results to: {out_csv}\")\n",
        "# print(f\"RUN_DIR used: {RUN_DIR}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "FLSimulation",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}